# -*- coding: utf-8 -*-
"""00-gymasium-tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ibDAKltGPhV0Ra9sUHgEdj9_BNI4Gle2

# Gymnasium Tutorial
* gymnasium: 강화학습 모델 연구를 위한 표준 강화학습 환경 API
* The Farama Foundation이 OpenAI의 gym을 이어서 관리

## 1. Setup
"""

# # 설치
# !pip install gymnasium[box2d]
# !pip install moviepy

# import
import gymnasium as gym
# from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder

"""## 2. Env
* https://gymnasium.farama.org/environments/box2d/lunar_lander/
"""

# 환경 정의
env = gym.make("MountainCar-v0", render_mode="human")

# # 환경 custom 정의
# env = gym.make(
#     "LunarLander-v2",
#     continuous = False,
#     gravity = -10.0,
#     enable_wind = False,
#     wind_power = 15.0,
#     turbulence_power = 1.5,
# )


# 환경 초기화
observation, info = env.reset(seed=42)

# 임의의 action으로 episode 진행
# observation(np.ndarray): 환경에 의해 관측된 state
# reward(float): 환경에 의해 주어진 행동에 해단 보상
# terminated(bool): 게임의 종료 여부 (terminal state 도달 여부)
# truncated(bool): 게임의 max_episode_steps 도달 여부
# info(dict): 부가 정보

for st in range(1000):
    action = env.action_space.sample()
    observation, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        print(st, terminated, truncated)
        break

"""## 3. Visualize"""

env = gym.make("MountainCar-v0", render_mode="human")

# video = VideoRecorder(env, 'video.mp4')

observation, info = env.reset(seed=42)
for st in range(1000):
    env.render()
    # video.capture_frame()

    action = env.action_space.sample()
    observation, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        break

# video.close()
env.close()

"""## 4. Pipeline"""

# import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
from collections import deque

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical

class Agent(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim):
        super().__init__()
        self.log_probs = []
        self.rewards = []

        self.module = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=1)
        )

    def forward(self, state):
        return self.module(state)

    def act(self, state):
        probs = self(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob

# hyperparameters
device = 'cpu'
gamma = 0.99
hidden_dim = 32
lr = 1e-2

# 환경 정의
env = gym.make(
    "MountainCar-v0",
    max_episode_steps = 300,
    render_mode = 'rgb_array'
)

# agent and optimizer
agent = Agent(env.observation_space.shape[0], env.action_space.n, hidden_dim).to(device)
optimizer = torch.optim.Adam(agent.parameters(), lr=lr)

ep_rewards = []
pbar = tqdm(range(1000))
for ep in pbar:
    # 환경 초기화
    ep_reward = 0
    state, info = env.reset(seed=42)

    # 에피소드 실행
    while True:
        state = torch.tensor(state).unsqueeze(0).to(device)
        action, log_prob = agent.act(state)
        state, reward, terminated, truncated, info = env.step(action)

        agent.log_probs.append(log_prob)
        agent.rewards.append(reward)

        ep_reward += reward
        if terminated or truncated:
            ep_rewards.append(ep_reward)
            pbar.set_postfix({'ep_reward': ep_reward})
            break


    # 학습
    R = 0
    returns = []
    for r in agent.rewards[::-1]:
        R = r + gamma * R
        returns.append(R)
    returns = returns[::-1]


    loss = 0.
    for log_prob, R in zip(agent.log_probs, returns):
        loss += -log_prob * R

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    agent.log_probs.clear()
    agent.rewards.clear()


torch.save(agent.state_dict(), 'ckpt.pt')

plt.title('episode reward')
plt.plot(ep_rewards)
plt.savefig('episode_reward.png')
plt.show()

agent.load_state_dict(torch.load('ckpt.pt'))

state, info = env.reset(seed=42)
while True:
    env.render()
    # video.capture_frame()

    state = torch.tensor(state).unsqueeze(0).to(device)
    with torch.no_grad():
        action, log_prob = agent.act(state)
    state, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        break

env.close()

